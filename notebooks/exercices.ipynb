{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HanaIstfalen/exam_2025/blob/main/notebooks/exercices.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_CACBFsndOCo"
      },
      "source": [
        "# Exercices"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Préliminaires**: Clone de votre repo et imports"
      ],
      "metadata": {
        "id": "hfkMtaHleKAa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! git clone https://github.com/HanaIstfalen/exam_2025.git\n",
        "! cp exam_2025/utils/utils_exercices.py .\n",
        "\n",
        "import copy\n",
        "import numpy as np\n",
        "import torch"
      ],
      "metadata": {
        "id": "xiD_cI-geJjI",
        "outputId": "24b776b5-e79c-4d61-d8f2-0baff03b863a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'exam_2025'...\n",
            "remote: Enumerating objects: 59, done.\u001b[K\n",
            "remote: Counting objects: 100% (59/59), done.\u001b[K\n",
            "remote: Compressing objects: 100% (51/51), done.\u001b[K\n",
            "remote: Total 59 (delta 21), reused 20 (delta 5), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (59/59), 1.41 MiB | 12.87 MiB/s, done.\n",
            "Resolving deltas: 100% (21/21), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Clef personnelle pour la partie théorique**\n",
        "\n",
        "Dans la cellule suivante, choisir un entier entre 100 et 1000 (il doit être personnel). Cet entier servira de graine au générateur de nombres aléatoire a conserver pour tous les exercices.\n",
        "\n"
      ],
      "metadata": {
        "id": "J3ga_6BNc5DR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mySeed = 200"
      ],
      "metadata": {
        "id": "PrCTHM4od5UZ"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\\\n",
        "\n",
        "---\n",
        "\n",
        "\\"
      ],
      "metadata": {
        "id": "TRWBLVpCWC06"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t5RcggmAkJLV"
      },
      "source": [
        "\\\n",
        "\n",
        "**Exercice 1** *Une relation linéaire*\n",
        "\n",
        "La fonction *generate_dataset* fournit deux jeux de données (entraînement et test). Pour chaque jeu de données, la clef 'inputs' donne accès à un tableau numpy (numpy array) de prédicteurs empilés horizontalement : chaque ligne $i$ contient trois prédicteurs $x_i$, $y_i$ et $z_i$. La clef 'targets' renvoie le vecteur des cibles $t_i$. \\\n",
        "\n",
        "Les cibles sont liées aux prédicteurs par le modèle:\n",
        "$$ t = \\theta_0 + \\theta_1 x + \\theta_2 y + \\theta_3 z + \\epsilon$$ où $\\epsilon \\sim \\mathcal{N}(0,\\eta)$\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from utils_exercices import generate_dataset, Dataset1\n",
        "train_set, test_set = generate_dataset(mySeed)"
      ],
      "metadata": {
        "id": "gEQmgTI8my8i"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q1** Par quelle méthode simple peut-on estimer les coefficients $\\theta_k$ ? La mettre en oeuvre avec la librairie python de votre choix."
      ],
      "metadata": {
        "id": "q5XZTrXNk12K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Une méthode simple pour estimer les coefficients θk est la\n",
        "# méthode des moindres carrés ordinaires\n",
        "X = train_set['inputs']\n",
        "y = train_set['targets']\n",
        "\n",
        "X = np.c_[np.ones(X.shape[0]), X]\n",
        "\n",
        "theta = np.linalg.solve(X.T @ X, X.T @ y)\n",
        "\n",
        "print(\"Coefficients estimés (θ) :\", theta)"
      ],
      "metadata": {
        "id": "HITtUqHhFMkn",
        "outputId": "4a9bfae1-98ca-4a5b-be08-3b68ffdb5401",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Coefficients estimés (θ) : [10.07876403  1.95156862  1.94842221  3.59966699]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "MXGXg8tlPULY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q2** Dans les cellules suivantes, on se propose d'estimer les $\\theta_k$ grâce à un réseau de neurones entraîné par SGD. Quelle architecture s'y prête ? Justifier en termes d'expressivité et de performances en généralisation puis la coder dans la cellule suivante."
      ],
      "metadata": {
        "id": "CH_Z5ZEIlQPE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Architecture : Un réseau de neurones monocouche avec une fonction d'activation linéaire.\n",
        "# Expressivité : Ce modèle correspond à un perceptron multicouche réduit à une seule couche (SLP).\n",
        "# Avec une fonction d'activation linéaire, il capture parfaitement la relation t = θ0 + θ1x + θ2y + θ3z + ε,\n",
        "# car cette relation est une combinaison linéaire des prédicteurs.\n",
        "# Généralisation : Ce modèle est simple et efficace pour cette tâche car il utilise exactement 4 paramètres (θ0, θ1, θ2, θ3),\n",
        "# ce qui est suffisant pour estimer une relation linéaire sans surajuster.\n"
      ],
      "metadata": {
        "id": "zrSKqfyJW99Z"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "# Dataset et dataloader :\n",
        "dataset = Dataset1(train_set['inputs'], train_set['targets'])\n",
        "dataloader = torch.utils.data.DataLoader(dataset, batch_size=100, shuffle=True)\n",
        "\n",
        "# A coder :\n",
        "class SimpleNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SimpleNet, self).__init__()\n",
        "        self.linear = nn.Linear(3, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.linear(x)"
      ],
      "metadata": {
        "id": "PPx543blnxdb"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q3** Entraîner cette architecture à la tâche de régression définie par les entrées et sorties du jeu d'entraînement (compléter la cellule ci-dessous)."
      ],
      "metadata": {
        "id": "g6BSTBitpGBD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize model, loss, and optimizer\n",
        "mySimpleNet = SimpleNet()\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = torch.optim.SGD(mySimpleNet.parameters(), lr=0.01)\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 500\n",
        "for epoch in range(num_epochs):\n",
        "    for batch_inputs, batch_targets in dataloader:\n",
        "\n",
        "      optimizer.zero_grad()\n",
        "      outputs = mySimpleNet(batch_inputs)\n",
        "      loss = criterion(outputs, batch_targets.unsqueeze(1))\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "    if (epoch + 1) % 50 == 0:\n",
        "        print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {loss.item():.4f}')\n"
      ],
      "metadata": {
        "id": "Wjfa2Z4RoPO-",
        "outputId": "c69bda4f-9b59-4b82-9c03-d5a233d62620",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [50/500], Loss: 4.3438\n",
            "Epoch [100/500], Loss: 4.6993\n",
            "Epoch [150/500], Loss: 4.3906\n",
            "Epoch [200/500], Loss: 3.5670\n",
            "Epoch [250/500], Loss: 3.5301\n",
            "Epoch [300/500], Loss: 3.6811\n",
            "Epoch [350/500], Loss: 3.6629\n",
            "Epoch [400/500], Loss: 3.4460\n",
            "Epoch [450/500], Loss: 4.6915\n",
            "Epoch [500/500], Loss: 5.8766\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q4** Où sont alors stockées les estimations des  $\\theta_k$ ? Les extraire du réseau *mySimpleNet* dans la cellule suivante."
      ],
      "metadata": {
        "id": "OZwKogEEp2Fr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Les estimations des θk sont stockées dans les paramètres de la couche linéaire (self.linear) du modèle mySimpleNet.\n",
        "# Extraction des poids et du biais\n",
        "poids = mySimpleNet.linear.weight.data\n",
        "biais = mySimpleNet.linear.bias.data\n",
        "\n",
        "print(\"θ0 estimé (biais) :\", biais.item())\n",
        "print(\"θ1, θ2, θ3 estimés (poids) :\", poids.tolist())"
      ],
      "metadata": {
        "id": "EjgWp1y1rseb",
        "outputId": "387701b4-87ab-4ba2-8165-a2d6580920d2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "θ0 estimé (biais) : 10.07835865020752\n",
            "θ1, θ2, θ3 estimés (poids) : [[1.949466347694397, 1.948156476020813, 3.5991649627685547]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q5** Tester ces estimations sur le jeu de test et comparer avec celles de la question 1. Commentez."
      ],
      "metadata": {
        "id": "pEB-V-oOrJED"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# --- Données de test ---\n",
        "X_test = test_set['inputs']\n",
        "y_test = test_set['targets']\n",
        "\n",
        "# --- Coefficients MCO (de la question 1) ---\n",
        "theta_ols = [10.07876403, 1.95156862, 1.94842221, 3.59966699]\n",
        "\n",
        "# --- Prédictions avec le réseau de neurones ---\n",
        "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
        "with torch.no_grad():\n",
        "    predictions_nn = mySimpleNet(X_test_tensor)\n",
        "predictions_nn = predictions_nn.numpy().squeeze()\n",
        "\n",
        "# --- Prédictions avec les MCO ---\n",
        "X_test_ols = np.c_[np.ones(X_test.shape[0]), X_test]\n",
        "predictions_ols = X_test_ols @ theta_ols\n",
        "\n",
        "# --- Calcul des MSE ---\n",
        "mse_nn = np.mean((predictions_nn - y_test)**2)  # MSE pour le réseau de neurones\n",
        "mse_ols = np.mean((predictions_ols - y_test)**2) # MSE pour les MCO\n",
        "\n",
        "# --- Affichage des résultats ---\n",
        "print(\"MSE pour les MCO :\", mse_ols)\n",
        "print(\"MSE pour le réseau de neurones :\", mse_nn)"
      ],
      "metadata": {
        "id": "M9iWlztja_Me",
        "outputId": "3962375b-20bd-4e30-edc2-0d0ab9c327d5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MSE pour les MCO : 4.009260943786592\n",
            "MSE pour le réseau de neurones : 4.009683638890263\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Les résultats des deux méthodes d'estimation sont très proches. Cela indique que le réseau de neurones a réussi à apprendre la relation linéaire entre les prédicteurs et la cible de manière efficace."
      ],
      "metadata": {
        "id": "Q5nIp42RcdON"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\\\n",
        "\n",
        "---\n",
        "\n",
        "\\"
      ],
      "metadata": {
        "id": "VvV2jIrBNtzf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Exercice 2** *Champ réceptif et prédiction causale*"
      ],
      "metadata": {
        "id": "CpRvXCaAtsIN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Le réseau défini dans la cellule suivante est utilisé pour faire le lien entre les valeurs $(x_{t' \\leq t})$ d'une série temporelle d'entrée et la valeur présente $y_t$ d'une série temporelle cible."
      ],
      "metadata": {
        "id": "8JG9wTfK5TBd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from utils_exercices import Outconv, Up_causal, Down_causal\n",
        "\n",
        "class Double_conv_causal(nn.Module):\n",
        "    '''(conv => BN => ReLU) * 2, with causal convolutions that preserve input size'''\n",
        "    def __init__(self, in_ch, out_ch, kernel_size=3, dilation=1):\n",
        "        super(Double_conv_causal, self).__init__()\n",
        "        self.kernel_size = kernel_size\n",
        "        self.dilation = dilation\n",
        "        self.conv1 = nn.Conv1d(in_ch, out_ch, kernel_size=kernel_size, padding=0, dilation=dilation)\n",
        "        self.bn1 = nn.BatchNorm1d(out_ch)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.conv2 = nn.Conv1d(out_ch, out_ch, kernel_size=kernel_size, padding=0, dilation=dilation)\n",
        "        self.bn2 = nn.BatchNorm1d(out_ch)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.pad(x, ((self.kernel_size - 1) * self.dilation, 0))\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu(x)\n",
        "\n",
        "        x = F.pad(x, ((self.kernel_size - 1) * self.dilation, 0))\n",
        "        x = self.conv2(x)\n",
        "        x = self.bn2(x)\n",
        "        x = self.relu(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class causalFCN(nn.Module):\n",
        "    def __init__(self, dilation=1):\n",
        "        super(causalFCN, self).__init__()\n",
        "        size = 64\n",
        "        n_channels = 1\n",
        "        n_classes = 1\n",
        "        self.inc = Double_conv_causal(n_channels, size)\n",
        "        self.down1 = Down_causal(size, 2*size)\n",
        "        self.down2 = Down_causal(2*size, 4*size)\n",
        "        self.down3 = Down_causal(4*size, 8*size, pooling_kernel_size=5, pooling_stride=5)\n",
        "        self.down4 = Down_causal(8*size, 4*size, pooling=False, dilation=2)\n",
        "        self.up2 = Up_causal(4*size, 2*size, kernel_size=5, stride=5)\n",
        "        self.up3 = Up_causal(2*size, size)\n",
        "        self.up4 = Up_causal(size, size)\n",
        "        self.outc = Outconv(size, n_classes)\n",
        "        self.n_classes = n_classes\n",
        "\n",
        "    def forward(self, x):\n",
        "        x1 = self.inc(x)\n",
        "        x2 = self.down1(x1)\n",
        "        x3 = self.down2(x2)\n",
        "        x4 = self.down3(x3)\n",
        "        x5 = self.down4(x4)\n",
        "        x = self.up2(x5, x3)\n",
        "        x = self.up3(x, x2)\n",
        "        x = self.up4(x, x1)\n",
        "        x = self.outc(x)\n",
        "        return x\n",
        "\n",
        "# Exemple d'utilisation\n",
        "model = causalFCN()\n",
        "# Série temporelle d'entrée (x_t):\n",
        "input_tensor1 = torch.rand(1, 1, 10000)\n",
        "# Série temporelle en sortie f(x_t):\n",
        "output = model(input_tensor1)\n",
        "print(output.shape)"
      ],
      "metadata": {
        "id": "fIbU1EJT1MM9",
        "outputId": "4ea8f034-1b0a-4789-c5a7-ee4a2776c653",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 1, 10000])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q1** De quel type de réseau de neurones s'agit-il ? Combien de paramètres la couche self.Down1 compte-t-elle (à faire à la main) ?\n",
        "Combien de paramètres le réseau entier compte-t-il (avec un peu de code) ?"
      ],
      "metadata": {
        "id": "-mNnsYU-7R7N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Le réseau de neuronnes présenté est un Fully Convolutional Network (FCN), plus précisément une variante causale de celui-ci."
      ],
      "metadata": {
        "id": "CF_3Yu-ZdSw8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Nombre de paramètres dans self.conv1: out_ch * (in_ch * kernel_size + 1)\n",
        "\n",
        "Nombre de paramètres dans self.conv1: out_ch * (out_ch * kernel_size + 1)\n",
        "\n",
        "Nombre de paramètres dans self.bn1: 2 * out_ch\n",
        "\n",
        "Nombre de paramètres dans self.bn2: 2 * out_ch\n",
        "\n",
        "on a size = 64, kernel_size = 3, in_ch_conv1 = 1, out_ch_conv1 = in_ch_conv2 = 128, out_ch_conv2 = 128\n",
        "\n",
        "Ainsi, le nombre de paramètres de la couche self.Down1 est la somme de tous ces paramètres : 50432"
      ],
      "metadata": {
        "id": "hrumL8IHeePH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Nb de paramètres au total:\n",
        "def count_parameters(model):\n",
        "  return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "model = causalFCN()\n",
        "total_params = count_parameters(model)\n",
        "print(f\"Nombre total de paramètres du modèle : {total_params}\")"
      ],
      "metadata": {
        "id": "qlYxUf6U9vH1",
        "outputId": "e4fb741d-e82f-452e-b908-0520a8e05acb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Nombre total de paramètres du modèle : 2872641\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q2** Par quels mécanismes la taille du vecteur d'entrée est-elle réduite ? Comment est-elle restituée dans la deuxième partie du réseau ?"
      ],
      "metadata": {
        "id": "I4D46A0-8LaV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Mécanismes de réduction de la taille : Convolutions causales sans padding et MaxPooling (down3)\n",
        "\n",
        "Mécanismes de restitution de la taille : UpSampling (up2) et Concaténation (up2, up3, up4)"
      ],
      "metadata": {
        "id": "hWb7Va_ZhVgi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q3** Par quels mécanismes le champ réceptif est-il augmenté ? Préciser par un calcul la taille du champ réceptif en sortie de *self.inc*."
      ],
      "metadata": {
        "id": "SVNeFnm88yV2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Mécanismes d'augmentation du champ réceptif:\n",
        "\n",
        "* Convolutions empilées: Chaque couche convolutive augmente le champ réceptif, permettant au réseau de \"voir\" une portion plus large de l'entrée.\n",
        "* Dilatations: Les dilatations augmentent l'espacement entre les éléments du noyau de convolution, élargissant le champ réceptif sans augmenter le nombre de paramètres.\n",
        "\n",
        "Taille du champ réceptif en sortie de self.inc : self.inc est composé de deux couches convolutives causales avec kernel_size=3 et dilation=1.\n",
        "Chaque couche augmente le champ réceptif de kernel_size - 1 = 2.\n",
        "Donc, le champ réceptif en sortie de self.inc est de 2 + 2 = 4."
      ],
      "metadata": {
        "id": "KxidCn2lh-gH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q4** Par un bout de code, déterminer empiriquement la taille du champ réceptif associé à la composante $y_{5000}$ du vecteur de sortie. (Indice: considérer les sorties associées à deux inputs qui ne diffèrent que par une composante...)"
      ],
      "metadata": {
        "id": "TVVcBPuA9EP0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_tensor1 = torch.zeros(1, 1, 10000)\n",
        "input_tensor2 = input_tensor1.clone()\n",
        "input_tensor2[0, 0, 0] = 1  # Modifier une seule composante\n",
        "\n",
        "with torch.no_grad():\n",
        "    output1 = model(input_tensor1)\n",
        "    output2 = model(input_tensor2)\n",
        "\n",
        "diff = output2 - output1\n",
        "\n",
        "receptive_field_indices = torch.nonzero(diff[0, 0]).squeeze()\n",
        "\n",
        "receptive_field_size = receptive_field_indices[-1].item() + 1\n",
        "\n",
        "print(\"Taille du champ réceptif pour y5000 :\", receptive_field_size)"
      ],
      "metadata": {
        "id": "69WMWCSZAg5_",
        "outputId": "90981425-c5f6-4728-c3fc-6387521fe88d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Taille du champ réceptif pour y5000 : 10000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q5** $y_{5000}$ dépend-elle des composantes $x_{t, \\space t > 5000}$ ? Justifier de manière empirique puis préciser la partie du code de Double_conv_causal qui garantit cette propriété de \"causalité\" en justifiant.  \n",
        "\n"
      ],
      "metadata": {
        "id": "gZ37skwm-Vpv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Non, y5000 ne dépend pas des composantes x_t pour t > 5000.\n",
        "\n",
        "Justification empirique : e champ réceptif pour y5000 est de 10000, ce qui signifie qu'il couvre l'ensemble de la séquence d'entrée de x0 à x9999. Cependant, le modèle est causal, ce qui implique que la sortie à un instant donné ne dépend que des entrées passées et présentes. Par conséquent, y5000 ne peut pas dépendre des composantes x_t pour t > 5000, car ces composantes représentent des entrées futures par rapport à l'instant 5000.\n",
        "\n",
        "La partie du code garantissant la causalité dans Double_conv_causal est le padding :\n",
        "\n",
        "x = F.pad(x, ((self.kernel_size - 1) * self.dilation, 0))\n",
        "\n",
        "Ce padding ajoute des zéros uniquement du côté gauche de la séquence d'entrée, ce qui assure que la convolution ne prend en compte que les valeurs passées et présentes de l'entrée pour calculer la sortie."
      ],
      "metadata": {
        "id": "nbgCzT0Ei9Bq"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PeooRYE-ATGt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\\\n",
        "\n",
        "---\n",
        "\n",
        "\\"
      ],
      "metadata": {
        "id": "qV52tusgNn6A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\\\n",
        "\n",
        "Exercice 3: \"Ranknet loss\""
      ],
      "metadata": {
        "id": "bm-sRzmfqc2m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Un [article récent](https://https://arxiv.org/abs/2403.14144) revient sur les progrès en matière de learning to rank. En voilà un extrait :"
      ],
      "metadata": {
        "id": "Wl8wUjsSM57D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "<img src=\"https://raw.githubusercontent.com/nanopiero/exam_2025/refs/heads/main/utils/png_exercice3.PNG?token=GHSAT0AAAAAAC427DACOPGNDNN6UDOLVLLAZ4BB2JQ\" alt=\"extrait d'un article\" width=\"800\">"
      ],
      "metadata": {
        "id": "SDZUXMlSDpoe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q1** Qu'est-ce que les auteurs appellent \"positive samples\" et \"negative samples\" ? Donner un exemple."
      ],
      "metadata": {
        "id": "9NzV1PbMNyuo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q2** Dans l'expression de $\\mathcal{L}_{RankNet}$, d'où proviennent les $z_i$ ? Que représentent-ils ?  "
      ],
      "metadata": {
        "id": "yIKQ5Eo9OnPq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q3** Pourquoi cette expression conduit-elle à ce que, après apprentissage, \"the estimated\n",
        "value of positive samples is greater than that of negative samples\n",
        "for each pair of positive/negative samples\" ?"
      ],
      "metadata": {
        "id": "r74fWiyvPb7Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q4** Dans le cadre d'une approche par deep learning, quels termes utilise-t-on pour qualifier les réseaux de neurones exploités et la modalité suivant laquelle ils sont entraînés ?"
      ],
      "metadata": {
        "id": "pk1EIi_VVi3R"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}